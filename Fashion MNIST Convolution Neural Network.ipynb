{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Processing QSTP\n",
    "\n",
    "### Dataset - MNIST Fashion\n",
    "### Model - 2 Layer CNN along with maxpool downscalling + 2 Connected Layers\n",
    "### Optimizer - SGD + Nesterov Momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms \n",
    "import torchvision.datasets as dsets\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "import gzip\n",
    "import urllib.request\n",
    "import os.path\n",
    "import torch.utils.data as data\n",
    "import codecs\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1. Create and Load Dataset\n",
    "\n",
    "### 1.1 Dataset Class of MNIST Fashion\n",
    "P.S. Please put files in the \"./data/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FashionMNISTDataset(data.Dataset):\n",
    "    '''Fashion MNIST Dataset'''\n",
    "    \n",
    "    urls = \t[\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "        'http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz'\n",
    "            ]\n",
    "\n",
    "    file_name =\t[\n",
    "                    'train-images-idx3-ubyte',\n",
    "                    'train-labels-idx1-ubyte',\n",
    "                    't10k-images-idx3-ubyte',\n",
    "                    't10k-labels-idx1-ubyte'\n",
    "                ]\n",
    "    \n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "    \n",
    "    def __init__(self, source , train=True, transform=None):\n",
    "        \n",
    "        self.source = source\n",
    "        \n",
    "        if os.path.exists(source) == False:\n",
    "            self.download(self.source)\n",
    "        \n",
    "        if self.check_processed(train=train, source=source) is not True:\n",
    "            self.process_files(train=train,source=source);\n",
    "        \n",
    "        if train is True:\n",
    "            self.X, self.Y = torch.load(\n",
    "                os.path.join(source, self.training_file))\n",
    "        else:\n",
    "            self.X, self.Y = torch.load(\n",
    "                os.path.join(source, self.test_file))\n",
    "        \n",
    "        self.transform = transform;\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X);\n",
    "    \n",
    "    def check_processed(self, train, source):\n",
    "        return os.path.exists(os.path.join(source, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(source, self.test_file))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.X[idx];\n",
    "        label = self.Y[idx];\n",
    "        \n",
    "        item = Image.fromarray(item.numpy(), mode='L')\n",
    "        \n",
    "        if self.transform:\n",
    "            item = self.transform(item);\n",
    "        \n",
    "        return (item, label);\n",
    "    \n",
    "    def download(self, source):\n",
    "        \n",
    "        try:\n",
    "\t\t\t      os.makedirs(self.source)\n",
    "        except OSError as exc:\n",
    "            if exc.errno != errno.EEXIST:\n",
    "              raise\n",
    "        pass\n",
    "        \n",
    "        for file_index in range(len(self.file_name)):\n",
    "            print(\"Downloading:\",self.urls[file_index])\n",
    "            urllib.request.urlretrieve(self.urls[file_index],(self.file_name[file_index]+'.gz'))\n",
    "            f = gzip.open(self.file_name[file_index]+'.gz', 'rb')\n",
    "            with open(source+self.file_name[file_index],'wb+') as w:\n",
    "                for line in f.readlines():\n",
    "                    w.write(line)\n",
    "            f.close()\n",
    "            os.remove(self.file_name[file_index]+\".gz\")\n",
    "    \n",
    "    @classmethod\n",
    "    def process_files(self, train, source):\n",
    "        \n",
    "        if train is True:\n",
    "            imgs='train-images-idx3-ubyte'\n",
    "            lbls='train-labels-idx1-ubyte'\n",
    "            n=60000\n",
    "        else:\n",
    "            imgs='t10k-images-idx3-ubyte'\n",
    "            lbls='t10k-labels-idx1-ubyte'\n",
    "            n=10000\n",
    "        \n",
    "        f = open(str(source+imgs), \"rb\")        \n",
    "        l = open(str(source+lbls), \"rb\")\n",
    "        \n",
    "        to_set = (\n",
    "            read_image_file(f),\n",
    "            read_label_file(l)\n",
    "        )\n",
    "        \n",
    "        if train is True:\n",
    "            fh = open(str(source+self.training_file), 'wb+')\n",
    "        else:\n",
    "            fh = open(str(source+self.test_file), 'wb+')\n",
    "        \n",
    "        torch.save(to_set, fh)\n",
    "        \n",
    "        fh.close()\n",
    "        \n",
    "        f.close()\n",
    "        l.close()\n",
    "\n",
    "        \n",
    "def get_int(b):\n",
    "    return int(codecs.encode(b, 'hex'), 16)\n",
    "\n",
    "\n",
    "def read_label_file(f):\n",
    "    data = f.read()\n",
    "    length = get_int(data[4:8])\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=8)\n",
    "    return torch.from_numpy(parsed).view(length).long()\n",
    "\n",
    "\n",
    "def read_image_file(f):\n",
    "    data = f.read()\n",
    "    length = get_int(data[4:8])\n",
    "    num_rows = get_int(data[8:12])\n",
    "    num_cols = get_int(data[12:16])\n",
    "    parsed = np.frombuffer(data, dtype=np.uint8, offset=16)\n",
    "    return torch.from_numpy(parsed).view(length, num_rows, num_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading: http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "train_dataset = FashionMNISTDataset(source='./data/',\n",
    "                                    train=True,\n",
    "                                    transform=transforms.ToTensor())\n",
    "test_dataset = FashionMNISTDataset(source='./data/',\n",
    "                                   train=False,\n",
    "                                   transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Make Dataset iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "no_iters = 12000\n",
    "no_epochs = no_iters / ( len(train_dataset) / batch_size )\n",
    "no_epochs = int(no_epochs)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        #Convolution 1\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.norm1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # Max pool 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Convolution 2\n",
    "        self.cnn2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.norm2 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Max pool 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #FNN 1\n",
    "        self.fc1 = nn.Linear(64*7*7, 64*7*7)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        \n",
    "        #Linear\n",
    "        self.fc2 = nn.Linear(64*7*7, 10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        #Convolution 1\n",
    "        out = self.cnn1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.norm1(out)\n",
    "        \n",
    "        #Max pool 1\n",
    "        out = self.maxpool1(out)\n",
    "        \n",
    "        #Convolution 2\n",
    "        out = self.cnn2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.norm2(out)\n",
    "        \n",
    "        #Max pool 2\n",
    "        out = self.maxpool2(out)\n",
    "        \n",
    "        #Resize\n",
    "        #original size: (100, 64, 7, 7)\n",
    "        #out.size(0): 100\n",
    "        #New out size: (100, 64*7*7)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        #FNN 1\n",
    "        out = self.fc1(out)\n",
    "        out = self.relu3(out)\n",
    "        \n",
    "        #Linear\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model =CNNModel().cuda()\n",
    "else:\n",
    "    model = CNNModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate Loss Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Instantiate Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "moment = 0.9\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum = moment, nesterov = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 2000, Loss: 0.17604094743728638, Train Accuracy: 94.92, Test Accuracy: 91.16\n",
      "Iteration: 4000, Loss: 0.04010731354355812, Train Accuracy: 98.23833333333333, Test Accuracy: 91.95\n",
      "Iteration: 6000, Loss: 0.035265859216451645, Train Accuracy: 99.66333333333333, Test Accuracy: 92.17\n",
      "Iteration: 8000, Loss: 0.0002535819949116558, Train Accuracy: 99.98333333333333, Test Accuracy: 92.37\n",
      "Iteration: 10000, Loss: 0.00018728256691247225, Train Accuracy: 100.0, Test Accuracy: 92.56\n",
      "Iteration: 12000, Loss: 0.0002989387430716306, Train Accuracy: 100.0, Test Accuracy: 92.57\n"
     ]
    }
   ],
   "source": [
    "iter = 1\n",
    "for epoch in range(no_epochs):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    for i,(images,labels) in enumerate(train_loader):\n",
    "        #Variables\n",
    "        if torch.cuda.is_available():\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            \n",
    "        #Clear Gradient\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #Outpus\n",
    "        outputs = model(images)\n",
    "        \n",
    "        #Calculate loss: softmax -> cross entropy loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        #Generate gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update Parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        iter += 1\n",
    "        if iter %2000 ==0:\n",
    "            #Calculate Accuracy\n",
    "            correct=0\n",
    "            total=0\n",
    "            train_accuracy = 0\n",
    "            test_accuracy = 0\n",
    "            \n",
    "            #Train Accuracy\n",
    "            for images,labels in train_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda()\n",
    "                #outputs\n",
    "                outputs = model(images)\n",
    "                \n",
    "                #Get predictions from maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                #Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #Total Correct Labels\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                    \n",
    "            train_accuracy = 100 * float(correct) / total\n",
    "            correct=0\n",
    "            total=0\n",
    "            \n",
    "            #Test Accuracy\n",
    "            for images,labels in test_loader:\n",
    "                if torch.cuda.is_available():\n",
    "                    images = images.cuda()\n",
    "                #outputs\n",
    "                outputs = model(images)\n",
    "                \n",
    "                #Get predictions from maximum value\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                \n",
    "                #Total number of labels\n",
    "                total += labels.size(0)\n",
    "                \n",
    "                #Total Correct Labels\n",
    "                if torch.cuda.is_available():\n",
    "                    correct += (predicted.cpu() == labels.cpu()).sum()\n",
    "                    \n",
    "            test_accuracy = 100 * float(correct) / total\n",
    "            \n",
    "            print('Iteration: {}, Loss: {}, Train Accuracy: {}, Test Accuracy: {}'.format(iter, loss.item(), train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
